import numpy as np
import torch
import torchaudio
import click
import gradio as gr
import tempfile
import gc
import traceback
import os
from slicer import Slicer

from infer import (
    load_models, 
    load_audio, 
    apply_fade, 
    process_segment
)

# Global variables for models
global svc_model, vocoder, rmvpe, hubert, rms_extractor, spk2idx, dataset_cfg, device
svc_model = vocoder = rmvpe = hubert = rms_extractor = spk2idx = dataset_cfg = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def initialize_models(model_path):
    global svc_model, vocoder, rmvpe, hubert, rms_extractor, spk2idx, dataset_cfg
    
    # Default to FP16, but will be overridden in processing
    use_fp16 = True
    
    # Clean up memory before loading models
    if svc_model is not None:
        del svc_model
        del vocoder
        del rmvpe
        del hubert
        del rms_extractor
        torch.cuda.empty_cache()
        gc.collect()
    
    try:
        # Check if the model file exists
        if not os.path.exists(model_path):
            return [], f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}"
        
        svc_model, vocoder, rmvpe, hubert, rms_extractor, spk2idx, dataset_cfg = load_models(model_path, device, use_fp16)
        available_speakers = list(spk2idx.keys())
        return available_speakers, f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ç”¨è¯´è¯äºº: {', '.join(available_speakers)}"
    except Exception as e:
        error_trace = traceback.format_exc()
        return [], f"âŒ åŠ è½½æ¨¡å‹å‡ºé”™: {str(e)}\n\nè¯¦ç»†ä¿¡æ¯: {error_trace}"

def process_with_progress(
    progress=gr.Progress(),
    input_audio=None,
    speaker=None,
    key_shift=0,
    infer_steps=32,
    robust_f0=1,
    use_fp16=True,
    # Advanced CFG parameters
    ds_cfg_strength=0.1,
    spk_cfg_strength=1.0,
    skip_cfg_strength=0.0,
    cfg_skip_layers=6,
    cfg_rescale=0.7,
    cvec_downsample_rate=2,
    # Slicer parameters
    slicer_threshold=-30.0,
    slicer_min_length=3000,
    slicer_min_interval=100,
    slicer_hop_size=10,
    slicer_max_sil_kept=200
):
    global svc_model, vocoder, rmvpe, hubert, rms_extractor, spk2idx, dataset_cfg
    
    # Fixed parameters
    target_loudness = -18.0
    restore_loudness = True
    fade_duration = 20.0
    sliced_inference = False
    
    # Input validation
    if input_audio is None:
        return None, "âŒ é”™è¯¯: æœªæä¾›è¾“å…¥éŸ³é¢‘ã€‚"
    
    if svc_model is None:
        return None, "âŒ é”™è¯¯: æ¨¡å‹æœªåŠ è½½ã€‚è¯·å…ˆåŠ è½½æ¨¡å‹ã€‚"
    
    if speaker is None or speaker not in spk2idx:
        return None, f"âŒ é”™è¯¯: æ— æ•ˆçš„è¯´è¯äººé€‰æ‹©ã€‚å¯ç”¨è¯´è¯äºº: {', '.join(spk2idx.keys())}"
    
    # Process the audio
    try:
        # Update status message
        progress(0, desc="å¤„ç†ä¸­: åŠ è½½éŸ³é¢‘...")
        
        # Convert speaker name to ID
        speaker_id = spk2idx[speaker]
        
        # Get config from loaded model
        hop_length = 512
        sample_rate = 44100
        
        # Handle negative skip_layers value as None
        if cfg_skip_layers < 0:
            cfg_skip_layers_value = None
        else:
            cfg_skip_layers_value = cfg_skip_layers
        
        # Load audio
        audio = load_audio(input_audio, sample_rate)
        
        # Initialize Slicer
        slicer = Slicer(
            sr=sample_rate,
            threshold=slicer_threshold,
            min_length=slicer_min_length,
            min_interval=slicer_min_interval,
            hop_size=slicer_hop_size,
            max_sil_kept=slicer_max_sil_kept
        )
        
        progress(0.1, desc="å¤„ç†ä¸­: åˆ‡åˆ†éŸ³é¢‘...")
        # Slice the input audio
        segments_with_pos = slicer.slice(audio)
        
        if not segments_with_pos:
            return None, "âŒ é”™è¯¯: åœ¨è¾“å…¥æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„éŸ³é¢‘ç‰‡æ®µã€‚"
        
        # Calculate fade size in samples
        fade_samples = int(fade_duration * sample_rate / 1000)
        
        # Process segments
        result_audio = np.zeros(len(audio) + fade_samples)  # Extra space for potential overlap
        
        progress(0.2, desc="å¤„ç†ä¸­: å¼€å§‹è½¬æ¢...")
        
        with torch.no_grad():
            for i, (start_sample, chunk) in enumerate(segments_with_pos):
                segment_progress = 0.2 + (0.7 * (i / len(segments_with_pos)))
                progress(segment_progress, desc=f"å¤„ç†ä¸­: ç‰‡æ®µ {i+1}/{len(segments_with_pos)}")
                
                # Process the segment
                audio_out = process_segment(
                    chunk, svc_model, vocoder, rmvpe, hubert, rms_extractor,
                    speaker_id, sample_rate, hop_length, device,
                    key_shift, infer_steps, ds_cfg_strength, spk_cfg_strength,
                    skip_cfg_strength, cfg_skip_layers_value, cfg_rescale,
                    cvec_downsample_rate, target_loudness, restore_loudness, sliced_inference,
                    robust_f0, use_fp16
                )
                
                # Ensure consistent length
                expected_length = len(chunk)
                if len(audio_out) > expected_length:
                    audio_out = audio_out[:expected_length]
                elif len(audio_out) < expected_length:
                    audio_out = np.pad(audio_out, (0, expected_length - len(audio_out)), 'constant')
                
                # Apply fades
                if i > 0:  # Not first segment
                    audio_out = apply_fade(audio_out.copy(), fade_samples, fade_in=True)
                    result_audio[start_sample:start_sample + fade_samples] *= \
                        np.linspace(1, 0, fade_samples)  # Fade out previous
                
                if i < len(segments_with_pos) - 1:  # Not last segment
                    audio_out[-fade_samples:] *= np.linspace(1, 0, fade_samples)  # Fade out
                
                # Add to result
                result_audio[start_sample:start_sample + len(audio_out)] += audio_out
                
                # Clean up memory after each segment
                torch.cuda.empty_cache()
        
        progress(0.9, desc="å¤„ç†ä¸­: å®ŒæˆéŸ³é¢‘...")
        # Trim any extra padding
        result_audio = result_audio[:len(audio)]
        
        # Create a temporary file to save the result
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            output_path = temp_file.name
        
        # Save output
        torchaudio.save(output_path, torch.from_numpy(result_audio).unsqueeze(0).float(), sample_rate)
        
        progress(1.0, desc="å¤„ç†å®Œæˆ!")
        return (sample_rate, result_audio), f"âœ… è½¬æ¢å®Œæˆ! å·²è½¬æ¢ä¸º **{speaker}** å¹¶è°ƒæ•´ **{key_shift}** ä¸ªåŠéŸ³ã€‚"
        
    except RuntimeError as e:
        # Handle CUDA out of memory errors
        if "CUDA out of memory" in str(e):
            # Clean up memory
            torch.cuda.empty_cache()
            gc.collect()
            
            return None, f"âŒ é”™è¯¯: å†…å­˜ä¸è¶³ã€‚è¯·å°è¯•æ›´çŸ­çš„éŸ³é¢‘æ–‡ä»¶æˆ–å‡å°‘æ¨ç†æ­¥éª¤ã€‚"
        else:
            return None, f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
    except Exception as e:
        error_trace = traceback.format_exc()
        return None, f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}\n\nè¯¦ç»†ä¿¡æ¯: {error_trace}"
    finally:
        # Clean up memory
        torch.cuda.empty_cache()
        gc.collect()

def create_ui():
    # CSS for better styling
    css = """
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    .container {
        max-width: 1200px;
        margin: auto;
    }
    .footer {
        margin-top: 20px;
        text-align: center;
        font-size: 0.9em;
        color: #666;
    }
    .title {
        text-align: center;
        margin-bottom: 10px;
    }
    .subtitle {
        text-align: center;
        margin-bottom: 20px;
        color: #666;
    }
    .button-primary {
        background-color: #5460DE !important;
    }
    .output-message {
        margin-top: 10px;
        padding: 10px;
        border-radius: 4px;
        background-color: #f8f9fa;
        border-left: 4px solid #5460DE;
    }
    .error-message {
        color: #d62828;
        font-weight: bold;
    }
    .success-message {
        color: #588157;
        font-weight: bold;
    }
    .info-box {
        background-color: #f8f9fa;
        border-left: 4px solid #5460DE;
        padding: 10px;
        margin: 10px 0;
        border-radius: 4px;
    }
    """
    
    # No need to initialize models automatically - let user provide their model path first
    available_speakers = []
    init_message = "â³ è¯·åŠ è½½æ¨¡å‹ä»¥å¼€å§‹ä½¿ç”¨ã€‚"
    
    with gr.Blocks(css=css, theme=gr.themes.Soft(), title="RIFT-SVC å£°éŸ³è½¬æ¢") as app:
        gr.HTML("""
        <div class="title">
            <h1>ğŸ¤ RIFT-SVC æ­Œå£°éŸ³è‰²è½¬æ¢</h1>
        </div>
        <div class="subtitle">
            <h3>ä½¿ç”¨ RIFT-SVC æ¨¡å‹å°†æ­Œå£°æˆ–è¯­éŸ³è½¬æ¢ä¸ºç›®æ ‡éŸ³è‰²</h3>
        </div>
        <div class="info-box">
            <p>ğŸ”— <strong>æƒ³è¦å¾®è°ƒè‡ªå·±çš„è¯´è¯äººï¼Ÿ</strong> è¯·è®¿é—® <a href="https://github.com/Pur1zumu/RIFT-SVC" target="_blank">RIFT-SVC GitHub ä»“åº“</a> è·å–å®Œæ•´çš„è®­ç»ƒå’Œå¾®è°ƒæŒ‡å—ã€‚</p>
        </div>
        <div class="info-box">
            <p>ğŸ“ <strong>æ³¨æ„ï¼š</strong> ä¸ºè·å¾—æœ€ä½³æ•ˆæœï¼Œè¯·ä½¿ç”¨èƒŒæ™¯å™ªéŸ³è¾ƒå°‘çš„å¹²å‡€éŸ³é¢‘ã€‚</p>
        </div>
        """)
        
        with gr.Row():
            # Left column (input parameters)
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ“¥ è¾“å…¥")
                    model_path = gr.Textbox(label="æ¨¡å‹è·¯å¾„", value="", placeholder="è¯·è¾“å…¥æ‚¨çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„", interactive=True)
                    reload_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", elem_id="reload_btn")
                    input_audio = gr.Audio(label="è¾“å…¥éŸ³é¢‘æ–‡ä»¶", type="filepath", elem_id="input_audio")
                
                with gr.Accordion("âš™ï¸ åŸºæœ¬å‚æ•°", open=True):
                    speaker = gr.Dropdown(label="ç›®æ ‡è¯´è¯äºº", interactive=True, elem_id="speaker")
                    key_shift = gr.Slider(minimum=-12, maximum=12, step=1, value=0, label="éŸ³è°ƒè°ƒæ•´ï¼ˆåŠéŸ³ï¼‰", elem_id="key_shift")
                    infer_steps = gr.Slider(minimum=8, maximum=64, step=1, value=32, label="æ¨ç†æ­¥æ•°", elem_id="infer_steps", 
                                           info="æ›´ä½çš„å€¼ = æ›´å¿«ä½†è´¨é‡è¾ƒä½ï¼Œæ›´é«˜çš„å€¼ = æ›´æ…¢ä½†è´¨é‡æ›´å¥½")
                    use_fp16 = gr.Checkbox(label="ä½¿ç”¨ FP16 ç²¾åº¦", value=True, info="å¯ç”¨ä»¥æé«˜æ€§èƒ½ï¼Œåœ¨æŸäº›GPUä¸Šå¯èƒ½ä¼šé™ä½ç²¾åº¦", elem_id="use_fp16")
                    robust_f0 = gr.Radio(choices=[0, 1, 2], value=1, label="éŸ³é«˜æ»¤æ³¢", 
                                        info="0=æ— ï¼Œ1=è½»åº¦è¿‡æ»¤ï¼Œ2=å¼ºåŠ›è¿‡æ»¤ï¼ˆæœ‰åŠ©äºè§£å†³æ–­éŸ³/ç ´éŸ³é—®é¢˜ï¼‰", 
                                        elem_id="robust_f0")
                
                with gr.Accordion("ğŸ”¬ é«˜çº§CFGå‚æ•°", open=True):
                    ds_cfg_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, value=0.1, 
                                               label="å†…å®¹å‘é‡å¼•å¯¼å¼ºåº¦", 
                                               info="æ›´é«˜çš„å€¼å¯ä»¥æ”¹å–„å†…å®¹ä¿ç•™å’Œå’¬å­—æ¸…æ™°åº¦ã€‚è¿‡é«˜ä¼šç”¨åŠ›è¿‡çŒ›ã€‚", 
                                               elem_id="ds_cfg_strength")
                    spk_cfg_strength = gr.Slider(minimum=0.0, maximum=2.0, step=0.01, value=1.0, 
                                                label="è¯´è¯äººå¼•å¯¼å¼ºåº¦", 
                                                info="æ›´é«˜çš„å€¼å¯ä»¥å¢å¼ºè¯´è¯äººç›¸ä¼¼åº¦ã€‚è¿‡é«˜å¯èƒ½å¯¼è‡´éŸ³è‰²å¤±çœŸã€‚", 
                                                elem_id="spk_cfg_strength")
                    skip_cfg_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, value=0.0, 
                                                 label="å±‚å¼•å¯¼å¼ºåº¦ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰", 
                                                 info="å¢å¼ºæŒ‡å®šå±‚çš„ç‰¹å¾æ¸²æŸ“ã€‚æ•ˆæœå–å†³äºç›®æ ‡å±‚çš„åŠŸèƒ½ã€‚", 
                                                 elem_id="skip_cfg_strength")
                    cfg_skip_layers = gr.Number(value=-1, label="CFGè·³è¿‡å±‚ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰", precision=0, 
                                               info="ç›®æ ‡å¢å¼ºå±‚ä¸‹æ ‡ï¼Œ-1ä¸ºç¦ç”¨æ­¤åŠŸèƒ½", 
                                               elem_id="cfg_skip_layers")
                    cfg_rescale = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, value=0.7, 
                                           label="CFGé‡ç¼©æ”¾å› å­", 
                                           info="çº¦æŸæ•´ä½“å¼•å¯¼å¼ºåº¦ã€‚å½“å¼•å¯¼æ•ˆæœè¿‡äºå¼ºçƒˆæ—¶ä½¿ç”¨è°ƒé«˜è¯¥å€¼ã€‚", 
                                           elem_id="cfg_rescale")
                    cvec_downsample_rate = gr.Radio(choices=[1, 2, 4, 8], value=2, 
                                                  label="ç”¨äºåå‘å¼•å¯¼çš„å†…å®¹å‘é‡ä¸‹é‡‡æ ·ç‡", 
                                                  info="æ›´é«˜çš„å€¼ï¼ˆå¯èƒ½ï¼‰å¯ä»¥æé«˜å†…å®¹æ¸…æ™°åº¦ã€‚", 
                                                  elem_id="cvec_downsample_rate")
                
                with gr.Accordion("âœ‚ï¸ åˆ‡ç‰‡å‚æ•°", open=False):
                    slicer_threshold = gr.Slider(minimum=-60.0, maximum=-20.0, step=0.1, value=-30.0, 
                                                label="é˜ˆå€¼ (dB)", 
                                                info="é™éŸ³æ£€æµ‹é˜ˆå€¼", 
                                                elem_id="slicer_threshold")
                    slicer_min_length = gr.Slider(minimum=1000, maximum=10000, step=100, value=3000, 
                                                 label="æœ€å°é•¿åº¦ (æ¯«ç§’)", 
                                                 info="æœ€å°ç‰‡æ®µé•¿åº¦", 
                                                 elem_id="slicer_min_length")
                    slicer_min_interval = gr.Slider(minimum=10, maximum=500, step=10, value=100, 
                                                   label="æœ€å°é™éŸ³é—´éš” (æ¯«ç§’)", 
                                                   info="åˆ†å‰²ç‰‡æ®µçš„æœ€å°é—´éš”", 
                                                   elem_id="slicer_min_interval")
                    slicer_hop_size = gr.Slider(minimum=1, maximum=20, step=1, value=10, 
                                              label="è·³è·ƒå¤§å° (æ¯«ç§’)", 
                                              info="ç‰‡æ®µæ£€æµ‹çª—å£å¤§å°", 
                                              elem_id="slicer_hop_size")
                    slicer_max_sil_kept = gr.Slider(minimum=10, maximum=1000, step=10, value=200, 
                                                  label="ä¿ç•™çš„æœ€å¤§é™éŸ³ (æ¯«ç§’)", 
                                                  info="ä¿ç•™åœ¨æ¯ä¸ªç‰‡æ®µè¾¹ç¼˜çš„æœ€å¤§é™éŸ³é•¿åº¦", 
                                                  elem_id="slicer_max_sil_kept")
            
            # Right column (output)
            with gr.Column(scale=1):
                convert_btn = gr.Button("ğŸµ è½¬æ¢å£°éŸ³", variant="primary", elem_id="convert_btn")
                gr.Markdown("### ğŸ“¤ è¾“å‡º")
                output_audio = gr.Audio(label="è½¬æ¢åçš„éŸ³é¢‘", elem_id="output_audio", autoplay=False, show_share_button=False)
                output_message = gr.Markdown(init_message, elem_id="output_message", elem_classes="output-message")
                
                gr.HTML("""
                <div class="info-box">
                    <h4>ğŸ” å¿«é€Ÿæç¤º</h4>
                    <ul>
                        <li><strong>éŸ³è°ƒè°ƒæ•´ï¼š</strong> ä»¥åŠéŸ³ä¸ºå•ä½ä¸Šè°ƒæˆ–ä¸‹è°ƒéŸ³é«˜ã€‚</li>
                        <li><strong>æ¨ç†æ­¥éª¤ï¼š</strong> æ­¥éª¤è¶Šå¤š = è´¨é‡è¶Šå¥½ä½†é€Ÿåº¦è¶Šæ…¢ã€‚</li>
                        <li><strong>éŸ³é«˜æ»¤æ³¢ï¼š</strong> æœ‰åŠ©äºæé«˜å…·æœ‰æŒ‘æˆ˜æ€§çš„éŸ³é¢‘ä¸­çš„éŸ³é«˜ç¨³å®šæ€§ã€‚</li>
                        <li><strong>CFGå‚æ•°ï¼š</strong> è°ƒæ•´è½¬æ¢è´¨é‡å’ŒéŸ³è‰²ã€‚</li>
                    </ul>
                </div>
                """)
        
        # Define button click events
        def load_model_and_update_speakers(model_path):
            # Call initialize_models to load the model
            available_speakers, message = initialize_models(model_path)
            
            # Explicitly update the dropdown with new speakers
            if available_speakers and len(available_speakers) > 0:
                return gr.update(choices=available_speakers, value=available_speakers[0]), message
            else:
                return gr.update(choices=[], value=None), message
        
        reload_btn.click(
            fn=load_model_and_update_speakers,
            inputs=[model_path],
            outputs=[speaker, output_message]
        )
        
        # Updated convert button click event
        convert_btn.click(
            fn=lambda: "â³ å¤„ç†ä¸­... è¯·ç¨å€™ã€‚",
            inputs=None,
            outputs=output_message,
            queue=False
        ).then(
            fn=process_with_progress,
            inputs=[
                input_audio, speaker, key_shift, infer_steps, robust_f0, use_fp16,
                ds_cfg_strength, spk_cfg_strength, skip_cfg_strength, cfg_skip_layers, cfg_rescale, cvec_downsample_rate,
                slicer_threshold, slicer_min_length, slicer_min_interval, slicer_hop_size, slicer_max_sil_kept
            ],
            outputs=[output_audio, output_message],
            show_progress_on=output_audio
        )
    
    return app

@click.command()
@click.option('--share', is_flag=True, help='Share the app')
def main(share=False):
    app = create_ui()
    app.launch(share=share)

if __name__ == "__main__":
    main()